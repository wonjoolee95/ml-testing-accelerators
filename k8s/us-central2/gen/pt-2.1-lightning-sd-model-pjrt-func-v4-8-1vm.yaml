# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"apiVersion": "batch/v1"
"kind": "CronJob"
"metadata":
  "labels":
    "accelerator": "v4-8"
    "benchmarkId": "pt-2.1-lightning-sd-model-pjrt-func-v4-8-1vm"
    "frameworkVersion": "pt-2.1"
    "mode": "func"
    "model": "lightning-sd-model-pjrt"
  "name": "pt-2.1-lightning-sd-model-pjrt-func-v4-8-1vm"
  "namespace": "automated"
"spec":
  "concurrencyPolicy": "Forbid"
  "jobTemplate":
    "metadata":
      "annotations":
        "ml-testing-accelerators/gcs-subdir": "pt-2.1/lightning-sd-model-pjrt/func/v4-8"
        "ml-testing-accelerators/metric-config": |
          {
            "sources": [
              {
                "tensorboard": {
                  "aggregate_assertions": [
                    {
                      "assertion": {
                        "inclusive_bounds": false,
                        "std_devs_from_mean": {
                          "comparison": "LESS",
                          "std_devs": 5
                        },
                        "wait_for_n_data_points": 10
                      },
                      "strategy": "FINAL",
                      "tag": "ExecuteTime__Percentile_99_sec"
                    },
                    {
                      "assertion": {
                        "inclusive_bounds": true,
                        "std_devs_from_mean": {
                          "comparison": "LESS",
                          "std_devs": 0
                        },
                        "wait_for_n_data_points": 0
                      },
                      "strategy": "FINAL",
                      "tag": "aten_ops_sum"
                    }
                  ],
                  "exclude_tags": [
                    "LearningRate"
                  ],
                  "include_tags": [
                    {
                      "strategies": [
                        "FINAL"
                      ],
                      "tag_pattern": "*"
                    }
                  ],
                  "merge_runs": true
                }
              }
            ]
          }
      "labels":
        "accelerator": "v4-8"
        "benchmarkId": "pt-2.1-lightning-sd-model-pjrt-func-v4-8-1vm"
        "frameworkVersion": "pt-2.1"
        "mode": "func"
        "model": "lightning-sd-model-pjrt"
    "spec":
      "activeDeadlineSeconds": 86400
      "backoffLimit": 0
      "template":
        "metadata":
          "annotations":
            "reserved.cloud-tpus.google.com": "false"
            "tf-version.cloud-tpus.google.com": "tpu-ubuntu2204-base"
        "spec":
          "activeDeadlineSeconds": 10800
          "containers":
          - "args": null
            "command":
            - "bash"
            - "-c"
            - |
              set -x
              set -u
              
              cat > workersetup.sh << TEST_SCRIPT_EOF
              sudo apt-get -y update
              // Ensure lock is released after udpate
              sudo kill -9 $(lsof /var/lib/dpkg/lock-frontend | awk '{print $2}')
              sudo dpkg --configure -a
              sudo apt-get -y install nfs-common
              sudo mkdir /datasets && sudo mount $(PYTORCH_DATA_LOCATION) /datasets
              
              yes '' | gcloud compute config-ssh
              
              cd
              pip3 install -U setuptools
              # `unattended-upgr` blocks us from installing apt dependencies
              sudo systemctl stop unattended-upgrades
              sudo apt-get -y update
              sudo apt install -y libopenblas-base
              # for huggingface tests
              sudo apt install -y libsndfile-dev
              # TODO change back to torch2.1 once pytorch released torch2.1
              # pip install --user \
              #   https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-2.1-cp310-cp310-linux_x86_64.whl \
              #   'torch_xla[tpuvm] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.1-cp310-cp310-linux_x86_64.whl'
              pip install --user --pre --no-deps torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu
              pip install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-nightly%2B20230825-cp310-cp310-linux_x86_64.whl
              pip install torch_xla[tpuvm]
              pip3 install pillow
              pip3 install typing_extensions
              pip3 install sympy
              git clone --depth=1 -b release/2.1 https://github.com/pytorch/pytorch.git
              cd pytorch
              git clone -b r2.1 https://github.com/pytorch/xla.git
              
              
              cd
              git clone https://github.com/pytorch-tpu/stable-diffusion.git
              cd stable-diffusion
              pip install transformers==4.19.2 diffusers invisible-watermark
              pip install -e .
              pip install torchmetrics==0.7.0
              pip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U
              pip install lmdb einops omegaconf
              pip install taming-transformers clip kornia==0.6 albumentations==0.4.3
              pip install starlette==0.27.0 && pip install tensorboard
              sudo apt-get update -y && sudo apt-get install libgl1 -y
              # wget -nv https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py
              pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers
              echo w | pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip
              # mv quantize.py ~/.local/lib/python3.8/site-packages/taming/modules/vqvae/
              
              # taming-transformers and CLIP override existing torch and torchvision so we need to reinstall
              # TODO change back to torch2.1 once pytorch released torch2.1
              pip uninstall -y torch torchvision
              pip3 install --user --pre --no-deps torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu
              pip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-nightly%2B20230825-cp310-cp310-linux_x86_64.whl
              pip install torch_xla[tpuvm]
              
              # Setup data
              wget -nv https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz
              tar -xf  imagenette2.tgz
              mkdir -p ~/.cache/autoencoders/data/ILSVRC2012_train/data
              mkdir -p ~/.cache/autoencoders/data/ILSVRC2012_validation/data
              mv imagenette2/train/*  ~/.cache/autoencoders/data/ILSVRC2012_train/data
              mv imagenette2/val/* ~/.cache/autoencoders/data/ILSVRC2012_validation/data
              
              # Get first stage model
              wget -nv -O models/first_stage_models/vq-f8/model.zip https://ommer-lab.com/files/latent-diffusion/vq-f8.zip
              cd  models/first_stage_models/vq-f8/
              unzip -o model.zip
              cd ~/stable-diffusion/
              
              # Fix syntax error
              sed -i 's/from torch._six import string_classes/string_classes = (str, bytes)/g' src/taming-transformers/taming/data/utils.py
              
              # Remove Checkpointing
              sed -i 's/trainer_kwargs\["callbacks"\]/# trainer_kwargs\["callbacks"\]/g' main_tpu.py
              
              echo 'export PATH=~/.local/bin:$PATH' >> ~/.bash_profile
              
              TEST_SCRIPT_EOF
              gcloud alpha compute tpus tpu-vm ssh xl-ml-test@$(cat /scripts/tpu_name) --zone=$(cat /scripts/zone) --ssh-key-file=/scripts/id_rsa --strict-host-key-checking=no --internal-ip --worker=all --command "$(cat workersetup.sh)"
              
              cat > testscript.sh << 'TEST_SCRIPT_EOF'
              export PJRT_DEVICE=TPU_C_API
              
              'python3' \
              'stable-diffusion/main_tpu.py' \
              '--train' \
              '--no-test' \
              '--base=stable-diffusion/configs/latent-diffusion/cin-ldm-vq-f8-ss.yaml' \
              '--' \
              'data.params.batch_size=32' \
              'lightning.trainer.max_epochs=5' \
              'model.params.first_stage_config.params.ckpt_path=stable-diffusion/models/first_stage_models/vq-f8/model.ckpt' \
              'lightning.trainer.enable_checkpointing=False' \
              'lightning.strategy.sync_module_states=False'
              TEST_SCRIPT_EOF
              gcloud alpha compute tpus tpu-vm ssh xl-ml-test@$(cat /scripts/tpu_name) --zone=$(cat /scripts/zone) --ssh-key-file=/scripts/id_rsa --strict-host-key-checking=no --internal-ip --worker=all --command "$(cat testscript.sh)"
              
              exit_code=$?
              bash /scripts/cleanup.sh
              exit $exit_code
            "env":
            - "name": "POD_NAME"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.name"
            - "name": "POD_UID"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.uid"
            - "name": "POD_NAMESPACE"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.namespace"
            - "name": "JOB_NAME"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.labels['job-name']"
            - "name": "MODEL_DIR"
              "value": "$(OUTPUT_BUCKET)/pt-2.1/lightning-sd-model-pjrt/func/v4-8/$(JOB_NAME)"
            - "name": "KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS"
              "value": "local"
            - "name": "LOCAL_OUTPUT_DIR"
              "value": "/tmp/model_dir"
            - "name": "XLA_USE_BF16"
              "value": "0"
            "envFrom":
            - "configMapRef":
                "name": "gcs-buckets"
            - "configMapRef":
                "name": "pytorch-nfs-ip"
            "image": "google/cloud-sdk"
            "imagePullPolicy": "Always"
            "lifecycle":
              "preStop":
                "exec":
                  "command":
                  - "bash"
                  - "/scripts/cleanup.sh"
            "name": "train"
            "resources":
              "limits":
                "tpu.googleapis.com/v4": 8
              "requests":
                "cpu": 1
                "memory": "2Gi"
            "volumeMounts":
            - "mountPath": "/scripts"
              "name": "scripts"
              "readOnly": false
            - "mountPath": "/dev/shm"
              "name": "dshm"
              "readOnly": false
          "initContainers":
          - "command":
            - "/bin/bash"
            - "-c"
            - |
              set -u
              set -e
              set -x
              
              project=$(curl -sS "http://metadata.google.internal/computeMetadata/v1/project/project-id" -H "Metadata-Flavor: Google")
              zone=$(curl -sS "http://metadata.google.internal/computeMetadata/v1/instance/zone" -H "Metadata-Flavor: Google" | awk -F'/' '{print $4}')
              tpu_name=tpu-${POD_UID}
              ssh-keygen -t rsa -f /scripts/id_rsa -q -N ""
              
              echo "
              gcloud alpha compute tpus tpu-vm delete -q --async ${tpu_name} --zone=${zone}
              sleep 60
              " > /scripts/cleanup.sh
              
              echo "xl-ml-test:$(cat /scripts/id_rsa.pub)" > ssh-keys.txt
              echo 'echo Running startup script' > startup-script.txt
              
              # Retry every 30 seconds for up to 10 minutes
              start_time="$(date -u +%s)"
              for i in {1..20}; do
                set +e
                gcloud alpha compute tpus tpu-vm create ${tpu_name} \
                  --accelerator-type='v4-8' \
                  --version='tpu-ubuntu2204-base'  \
                  --metadata-from-file='ssh-keys=ssh-keys.txt,startup-script=startup-script.txt' \
                  --labels='test-name=pt-2-1-lightning-sd-model-pjrt-func-v4-8-1vm' \
                  --zone=${zone}
              
                exit_code=$?
                set -e
              
                current_time="$(date -u +%s)"
                elapsed_seconds=$(($current_time-$start_time))
                # Break if command passed or 10-minute limit reached
                test $exit_code = 0 && break
                test $elapsed_seconds -gt 600 && break
                sleep 30
              done
              
              if [ $exit_code -ne 0 ]; then
                exit $exit_code
              fi
              
              echo ${zone} > /scripts/zone
              echo ${tpu_name} > /scripts/tpu_name
              gcloud compute tpus describe ${tpu_name} --project=${project} --zone=${zone} --format="value(networkEndpoints[0].ipAddress)" > /scripts/tpu_ip
              gcloud compute tpus describe ${tpu_name} --project=${project} --zone=${zone} --flatten="networkEndpoints[]" --format="csv[no-heading](networkEndpoints.ipAddress)" > /scripts/all_tpu_ips
              
              sleep 90
              
            "env":
            - "name": "POD_UID"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.uid"
            "image": "google/cloud-sdk"
            "name": "create-tpu"
            "volumeMounts":
            - "mountPath": "/scripts"
              "name": "scripts"
          "nodeSelector":
            "tpu-available": "true"
          "priorityClassName": "tpu-pod"
          "restartPolicy": "Never"
          "volumes":
          - "emptyDir":
              "medium": "Memory"
            "name": "scripts"
          - "emptyDir":
              "medium": "Memory"
            "name": "dshm"
  "schedule": "0 7 * * *"
  "successfulJobsHistoryLimit": 1